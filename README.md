# Local LLM Chatbot

A locally-running AI chatbot built with Chainlit and CTransformers. This chatbot provides a clean web interface for interacting with local LLM models, with all processing happening on your machine.

## Features

- ðŸ¤– Local LLM inference using CTransformers
- ðŸ’¬ Clean chat interface powered by Chainlit
- ðŸ”„ Conversation history tracking
- ðŸŽ¯ Focused responses with custom system prompt
- âš¡ Real-time streaming responses
- ðŸš€ Easy to deploy and use

## Quick Start

1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Run the chatbot:
```bash
chainlit run chat.py
```

3. Open your browser at `http://localhost:8000`

## Implementation Details

The chatbot is implemented with the following key components:

- **Model**: Uses the Orca Mini 3B model (quantized version) from Hugging Face
- **Interface**: Chainlit for the web UI and chat functionality
- **Memory**: Maintains conversation history for context-aware responses
- **Streaming**: Real-time token streaming for responsive user experience

The system prompt instructs the AI to provide helpful, concise answers to user queries.

## Project Structure

```
.
â”œâ”€â”€ chat.py           # Main chatbot implementation
â”œâ”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ chainlit.md      # Chainlit configuration
â””â”€â”€ assets/          # Project assets
```

## Development Environment

The project includes comprehensive development environment setup:

- VS Code devcontainer configuration
- Code quality tools:
  - Black formatter
  - Flake8 linter
  - MyPy type checking
- Git configuration and ignore rules

## Dependencies

Key packages:
- `ctransformers`: For local LLM inference
- `chainlit`: Web interface and chat functionality
- Additional development dependencies in `requirements.txt`

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
```

